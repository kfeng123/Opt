\documentclass[11pt,letterpaper]{article}
 
\usepackage{lineno,hyperref}


%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{galois} % composition function \comp
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{datetime}
\newdate{date}{6}{7}{2017}

\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}


\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{\quad\quad Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{\quad\quad Condition}

\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}


\title{Fano Method}
\author{Rui Wang}
\date{\displaydate{date}}
\begin{document}
\maketitle
\begin{abstract}
    123
\end{abstract}
\section{Basic framework}
Throughout, we let $\mathcal{P}$ denote a class of distributions on a sample space $\mathcal{X}$, and let $\theta:\mathcal{P}\to \Theta$ denote a function defined on $\mathcal{P}$, that is, a mapping $P\mapsto \theta(P)$.
The goal is to estimate the parameter $\theta(P)$ based on observations $X_i$ drawn from the distribution $P$.

To evaluate the quality of an estimator $\hat{\theta}$, we let $\rho:\Theta\times\Theta \to \mathbb{R}_{+}$ denote a semimetric on the space $\Theta$, which we use to measure the error of an estimator for the parameter $\theta$, and let $\Phi:\mathbb{R}_+\to \mathbb{R}_+$ be a non-decreasing function with $\Phi(0)=0$.

\paragraph{From estimation to testing}
Given an index set $\mathcal{V}$ of finite cardinality, consider a family of distributions $\{P_v\}_{v\in\mathcal{V}}$ contained within $\mathcal{P}$.
This family induces a collection of parameters $\{\theta(P_v)\}_{v\in\mathcal{V}}$; we call the family a $2\delta$-packing in the $\rho$-semimetric if 
$$
\rho(\theta(\theta(P_v),\theta(P_{v'})))\geq 2\delta \quad \textrm{for all $v\neq v'$}.
$$
We use this family to define the canonical hypothesis testing problem:
\begin{itemize}
    \item
        first, nature chooses $V$ according to the uniform distribution over $\mathcal{V}$;
    \item
        second, conditioned on the choice $V=v$, the random sample $X=X_1^n=(X_1,\ldots,X_n)$ is drawn from  the $n$-fold product distribution $P_v^n$.
\end{itemize}
\begin{proposition}
    The minimax error has lower bound
    $$
    \mathfrak{M}_n(\theta(\mathcal{P}),\Phi\comp \rho)\geq \Phi(\delta) \inf_{\Psi} \mathbb{P}(\Psi(X_1,\ldots,X_n)\neq V).
    $$
\end{proposition}

\section{Metric entropy and packing numbers}
\begin{itemize}
    \item
        Covering number: $N(\delta,\Theta,\rho)$
    \item
        Metric entropy: $\log N(\delta,\Theta,\rho)$
    \item
        Packing number: $M(\delta,\Theta,\rho)$
\end{itemize}
\begin{lemma}
    $M(2\delta,\Theta,\rho)\leq N(\delta,\Theta,\rho)\leq M(\delta,\Theta,\rho)$
\end{lemma}
\begin{lemma}[Gilbert-Vershamov bound]
    Let $d\geq 1$. There is a subset $\mathcal{V}$ of the $d$-dimensional hypercube $\mathcal{H}_d=\{-1,1\}^d$ of size $|\mathcal{V}|\geq \exp(d/8)$ such that the $\ell_1$-distance
    $$
    \|v-v'\|_1 =2\sum_{j=1}^d \mathbf{1}\{v_j\neq v_j'\}\geq \frac{d}{2}
    $$
    for all $v\neq v'$ with $v,v'\in \mathcal{V}$.
\end{lemma}
\begin{lemma}
    Let $\|\cdot\|$ be any norm in $\mathbb{R}^d$.
    Let $\mathbb{B}$ denote the unit $\|\cdot\|$-ball in $\mathbb{R}^d$. Then
    $$
    \left(\frac{1}{\delta}\right)^d
    \leq 
    N(\delta,\mathbb{B},\|\cdot\|)
    \leq
    \left(1+\frac{2}{\delta}\right)^d.
    $$
\end{lemma}

\section{Fano inequality}
Let $V$ be a random variable taking values in a finite set $\mathcal{V}$, and assume that we observe a random variable $X$, and then must estimate or guess the true value of $\hat{V}$. That is, we have the Markov chain
$$
V\rightarrow X\rightarrow \hat{V}.
$$
Let the function $h_2(p)=-p\log p-(1-p) \log (1-p)$ denote the binary entropy.
\begin{proposition}[Fano inequality] For any Markov chain $V\rightarrow X\rightarrow \hat{V}$, we have
    $$
    h_2(\Pr(\hat{V}\neq V))+\Pr(\hat{V}\neq V) \log (|\mathcal{V}|-1)\geq H(V|\hat{V}).
    $$
    \begin{proof}
        Let $E$ be the indicator for the event that $\hat{X}\neq X$, that is, $E=1$ if $\hat{V}\neq V$ and is $0$ otherwise.
        Then we have
        \begin{align*}
            &H(V|\hat{V})=H(V,E|\hat{V})=H(V|E,\hat{V})+H(E|\hat{V})\\
            =&\Pr(E=0)\underbrace{H(V|E=0,\hat{V})}_{0}+\Pr(E=1)H(V|E=1,\hat{V})+H(E|\hat{V})\\
            \leq& \Pr(E=1)\log(|\mathcal{V}|-1)+H(E)
        \end{align*}
    \end{proof}
\begin{remark}
During the proof, $X$ is not needed.
\end{remark}
    \begin{corollary}
        Assume $V$ is uniform on $\mathcal{V}$, then
        $$
        \Pr(\hat{V}\neq V)\geq 1-\frac{I(V;X)+\log 2}{\log(|\mathcal{V}|)}.
        $$
    \end{corollary}
    \begin{proof}
Note that $h_2(\Pr(\hat{V}\neq V))\leq \log 2$ and 
        \begin{align*}
            H(V|\hat{V})=H(V)-I(V;\hat{V})
            \geq H(V)-I(V;X)
            =\log(|\mathcal{V}|)-I(V;X).
        \end{align*}
    \end{proof}
    
\end{proposition}

\section{The classical (local) Fano method}
\begin{proposition}
    Let $\{\theta(P_v)\}_{v\in V}$ be a $2\delta$-packing in the $\rho$-semimetric. Assume that $V$ is uniform on the set $\mathcal{V}$, and conditional on $V=v$, we draw a sample $X\sim P_v$. Then the minimax risk has lower bound
    $$
    \mathfrak{M}(\theta(\mathcal{P});\Phi\comp \rho)\geq \Phi(\delta)\left(1-\frac{I(V;X)+\log 2}{\log |\mathcal{V}|}\right).
    $$
\end{proposition}


\section*{References}

\bibliography{mybibfile}

\end{document}
